{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1CO0S2X_K0b"
      },
      "source": [
        "# Replication\n",
        "\n",
        "I cant find model weights, only the layers of the final quanitzed model\n",
        "\n",
        "Instead I will replicate the architecture and the way training data was split etc\n",
        "\n",
        "Training Input is everything except classification\n",
        "\n",
        "Data is scaled in the following ways:\n",
        "- pT ([:,:,0]) Is z-scaled (x - mean)/std : They say they bring all to O(1) but not how so this seems reasonable\n",
        "- eta and phi are tanh scaled : tanh(x / x_b) where x_b is the max(abs(x)) for non padded entries : This is only done before the MSE loss, implementing this for KL is very non trivial\n",
        "\n",
        "Models are constructed as classes with the encoder and decoder created as submodels\n",
        "\n",
        "So if we have a full model say CNN_VAE we access the encoder with either CNN_VAE.layers[0] or CNN_VAE.get_layer(\"cnn_encoder\"), but check what the actual name is, the submodel extracted then has the same methods available\n",
        "\n",
        "The MSE loss and masked MSE loss is handled as functions that require a partial to work due to the way tensorflow works\n",
        "\n",
        "The beta parameter managing the balance between MSE and KL is not defined in the paper -> A number of values are trialed and the best is selected -> Note that this is non exhaustive of a search and only done to order of magnitude not to exact value\n",
        "\n",
        "Also note that if you run this and an error occurs during any step whatsoever you will likely have to restart the kernel as I am now convinced that this causes Memory leaks either due to jupyter loosing the reference or tensorflow handling the reference in an odd fashion -> I would like to blame tensorflow rather than jupyter\n",
        "\n",
        "Misc Notes:\n",
        "\n",
        "When you inevitably run into nan errors dont bother printing things use tf.debugging.enable_check_numerics() instead, this will tell the compiler to raise an exception on nan and inf with a full backtrace on which layer this occured, look for a long / seperated string that contains layer and operation names it essentially gives you the location, if its not clear copy paste with the model architecture to chatGPT it will usually identify the location (allthough it really likes quoting the obvious reasons nan may arrise that have nothing to do with the current problem)\n",
        "\n",
        "For debugging print works when not compiled, tf.print works when compiled, i always just do both and that works"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ignore this\n",
        "#!pip install tensorflow"
      ],
      "metadata": {
        "id": "-guq6HxdxjLl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x8mrcSU6_K0c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.mixed_precision import set_dtype_policy\n",
        "\n",
        "#setting the default evaluation for keras to 16 bit floats, unless more accuracy is necesarry. Is supposed to speed up learning for modern GPU's and TPU's.\n",
        "#All the hls4ml seem to set it to 16bit and i'm guessing there is some built in safety here. See \"https://keras.io/api/mixed_precision/\". Might need to be moved up?\n",
        "set_dtype_policy('mixed_float16')\n",
        "\n",
        "# Loss Functions --- Reasoning written out where data is loaded -> Partial needs to be called on these\n",
        "import tensorflow as tf\n",
        "@tf.function\n",
        "def scale_for_MSE(y_true, y_pred, eta_b, phi_b):\n",
        "    \"\"\"\n",
        "    Loss scaling helper\n",
        "    \"\"\"\n",
        "\n",
        "    #cast prediction as 32bit\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    # Extract\n",
        "    eta_true = y_true[:, :, 1]\n",
        "    phi_true = y_true[:, :, 2]\n",
        "    eta_pred = y_pred[:, :, 1]\n",
        "    phi_pred = y_pred[:, :, 2]\n",
        "    # Apply tanh scaling\n",
        "    eta_true_scaled = tf.tanh(eta_true / eta_b)\n",
        "    phi_true_scaled = tf.tanh(phi_true / phi_b)\n",
        "    eta_pred_scaled = tf.tanh(eta_pred / eta_b)\n",
        "    phi_pred_scaled = tf.tanh(phi_pred / phi_b)\n",
        "\n",
        "\n",
        "    # Replace and hope compilation makes this efficient\n",
        "    y_true_scaled = tf.concat([\n",
        "        y_true[:, :, :1],\n",
        "        eta_true_scaled[:, :, tf.newaxis],\n",
        "        phi_true_scaled[:, :, tf.newaxis],\n",
        "        y_true[:, :, 3:],\n",
        "    ], axis=-1)\n",
        "\n",
        "    y_pred_scaled = tf.concat([\n",
        "        y_pred[:, :, :1],\n",
        "        eta_pred_scaled[:, :, tf.newaxis],\n",
        "        phi_pred_scaled[:, :, tf.newaxis],\n",
        "        y_pred[:, :, 3:],\n",
        "    ], axis=-1)\n",
        "    return y_true_scaled, y_pred_scaled\n",
        "\n",
        "@tf.function\n",
        "def full_MSE(y_true, y_pred, eta_b, phi_b):\n",
        "    \"Full Loss Function overwrite\"\n",
        "    y_true_scaled, y_pred_scaled = scale_for_MSE(y_true, y_pred, eta_b, phi_b)\n",
        "    # Compute MSE\n",
        "    mse = tf.reduce_mean(\n",
        "        tf.reduce_sum(tf.square(y_true_scaled - y_pred_scaled), axis=(1, 2))\n",
        "    )\n",
        "\n",
        "    return mse\n",
        "\n",
        "def masked_mse(y_true_og, y_pred_og, eta_b, phi_b, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Computes MSE per event, ignoring any zero-padded rows.\n",
        "    Assumes y_true, y_pred each have shape (batch, n_features)\n",
        "    or possibly (batch, H, W, C). Adjust logic as needed.\n",
        "    \"\"\"\n",
        "    y_true, y_pred = scale_for_MSE(y_true_og, y_pred_og, eta_b, phi_b)\n",
        "    # Sum over feature-dims to see whether row is all zeros\n",
        "    row_sum = tf.reduce_sum(tf.abs(y_true), axis=-1)  # shape = (batch,)\n",
        "    # A mask: 1 if non-padded, 0 if padded\n",
        "    mask = tf.cast(row_sum > epsilon, tf.float32)\n",
        "    # Squared error, summation over features\n",
        "    sq_error = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)  # shape=(batch,)\n",
        "    # Apply mask -> I hate this\n",
        "    sq_error_masked = sq_error * mask\n",
        "    return tf.reduce_sum(sq_error_masked) / tf.reduce_sum(mask)\n",
        "\n",
        "def tmp():\n",
        "    \"\"\"For VAE implementation a lot of code is taken from\n",
        "    https://keras.io/examples/generative/vae/\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import keras\n",
        "    from keras import ops\n",
        "    from keras import layers\n",
        "    class Sampling(layers.Layer):\n",
        "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "        def __init__(self, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "        def call(self, inputs):\n",
        "            z_mean, z_log_var = inputs\n",
        "            batch = ops.shape(z_mean)[0]\n",
        "            dim = ops.shape(z_mean)[1]\n",
        "            epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator, dtype=tf.float16)\n",
        "            return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "    return Sampling\n",
        "Sampling = tmp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cWgF0mt-_K0d",
        "outputId": "7d249e65-392b-48e9-b3a0-9916ea81e098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"DNN_Encoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"DNN_Encoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_in (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m3\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast (\u001b[38;5;33mCast\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m3\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m1,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │             \u001b[38;5;34m528\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ latent (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ latent (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,435\u001b[0m (9.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,435</span> (9.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,435\u001b[0m (9.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,435</span> (9.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"DNN_Decoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"DNN_Decoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_in (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast_1 (\u001b[38;5;33mCast\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │              \u001b[38;5;34m64\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │              \u001b[38;5;34m64\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m544\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reconstruction (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57\u001b[0m)                  │           \u001b[38;5;34m1,881\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m3\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reconstruction (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,881</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,681\u001b[0m (10.47 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,681</span> (10.47 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,585\u001b[0m (10.10 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,585</span> (10.10 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "def dnn_reshape():\n",
        "    \"\"\"Input Space for DNN is flattened\"\"\"\n",
        "    return layers.Flatten()\n",
        "\n",
        "def dnn_encoder(input_dim = 57, latent_dim = 3, name='DNN_Encoder', vae=False):\n",
        "    inputs = tf.keras.Input(shape=(19,3,), name=\"encoder_in\")\n",
        "    x = inputs#layers.BatchNormalization()(inputs)\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    x = layers.Dense(32)(x) # Activation is None By Default\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dense(16)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    if vae:\n",
        "        z_mean = layers.Dense(latent_dim, name=\"z_mean\", dtype=tf.float32)(x)\n",
        "        # Kernel initializer as this term is exponentiated and default init makes the exp -> ooo\n",
        "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\", kernel_initializer=\"zeros\", dtype=tf.float32)(x)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "        return Model(inputs, [z_mean, z_log_var, z], name=name)\n",
        "\n",
        "    latent = layers.Dense(latent_dim, name=\"latent\")(x)\n",
        "\n",
        "    return Model(inputs, latent, name=name)\n",
        "\n",
        "def dnn_decoder(output_dim=57, latent_dim=3, name='DNN_Decoder'):\n",
        "    \"\"\"\n",
        "    BN between Fully connected and activation, last doesnt have activation & BN\n",
        "    \"\"\"\n",
        "    latent_in = tf.keras.Input(shape=(latent_dim,), name=\"decoder_in\")\n",
        "\n",
        "    y = layers.Dense(16)(latent_in)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    y = layers.Dense(32)(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    outputs = layers.Dense(output_dim, name=\"reconstruction\")(y)\n",
        "\n",
        "    # And reshape so that the loss works right\n",
        "    outputs = layers.Reshape((19,3,))(outputs)\n",
        "\n",
        "    return Model(latent_in, outputs, name=name)\n",
        "\n",
        "dnn_encoder().summary()\n",
        "dnn_decoder().summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pk7PaOKw_K0e"
      },
      "outputs": [],
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class DNN_AE(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    AE = Encoder -> Decoder, no sampling.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=57, latent_dim=3, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = dnn_encoder(input_dim=input_dim, latent_dim=latent_dim)\n",
        "        self.decoder = dnn_decoder(output_dim=input_dim, latent_dim=latent_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        z = self.encoder(x)            # shape=(batch, latent_dim)\n",
        "        x_recon = self.decoder(z)      # shape=(batch, input_dim)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class DNN_VAE(tf.keras.Model):\n",
        "    def __init__(self, eta_b=None, phi_b=None, input_dim=57, latent_dim=3, beta=1.0, **kwargs):\n",
        "        \"\"\"\n",
        "        Note eta and phi are absolutely required, however, the way I saved the model i need to do this\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = dnn_encoder(input_dim=input_dim, latent_dim=latent_dim, vae=True)\n",
        "        self.decoder = dnn_decoder(output_dim=input_dim, latent_dim=latent_dim)\n",
        "        self.beta = beta\n",
        "        self.eta_b = eta_b\n",
        "        self.phi_b = phi_b\n",
        "\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                masked_mse(data, reconstruction, self.eta_b, self.phi_b)\n",
        "            )\n",
        "\n",
        "            # ✅ KL divergence\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.cast(tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)), tf.float32)\n",
        "\n",
        "            # ✅ Total loss with β weighting\n",
        "            total_loss = (1.0 - self.beta) * reconstruction_loss + self.beta * kl_loss\n",
        "\n",
        "        # ✅ Backpropagation\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # ✅ Update metrics\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"\n",
        "        Same as above but no gradient updates\n",
        "        \"\"\"\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            masked_mse(data, reconstruction, self.eta_b, self.phi_b)\n",
        "        )\n",
        "\n",
        "        # ✅ KL divergence\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.cast(tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)), tf.float32)\n",
        "\n",
        "        # ✅ Total loss with β weighting\n",
        "        total_loss = (1.0 - self.beta) * reconstruction_loss + self.beta * kl_loss\n",
        "\n",
        "        # ✅ Update metrics\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l5Sw112r_K0f",
        "outputId": "93bddc64-7f08-4e1f-ceea-81d1f306a34d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cnn_encoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_encoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ cnn_encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast_2 (\u001b[38;5;33mCast\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ zero_padding2d (\u001b[38;5;33mZeroPadding2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │               \u001b[38;5;34m4\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m)           │             \u001b[38;5;34m144\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)                         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d (\u001b[38;5;33mAveragePooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m1,536\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_1                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ latent (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m264\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ cnn_encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ zero_padding2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_1                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ latent (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,948\u001b[0m (7.61 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,948</span> (7.61 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,946\u001b[0m (7.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,946</span> (7.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2\u001b[0m (8.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (8.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cnn_decoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_decoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ cnn_decoder_input (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast_3 (\u001b[38;5;33mCast\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m576\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m3,104\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ zero_padding2d_1 (\u001b[38;5;33mZeroPadding2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │           \u001b[38;5;34m1,552\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ zero_padding2d_2 (\u001b[38;5;33mZeroPadding2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │             \u001b[38;5;34m145\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ cnn_decoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ zero_padding2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,552</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ zero_padding2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,377\u001b[0m (21.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,377</span> (21.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,377\u001b[0m (21.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,377</span> (21.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def build_cnn_encoder(name='cnn_encoder', vae=False):\n",
        "    \"\"\"\n",
        "    Build CNN Encoder\n",
        "    vae : Variational Autoencoder varitation (second final layer)\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(19,3,1), name=\"cnn_encoder_input\")\n",
        "    #      Zeropad to (20,3,1)  - Syntax padding = ((top, bottom), (left, right))\n",
        "    x = layers.ZeroPadding2D(padding=((1,0),(0,0)))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    #      Block 1  -> Padding valid means no padding\n",
        "    x = layers.Conv2D(16, kernel_size=(3,3), strides=(1,1), padding='valid', use_bias=False)(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.AveragePooling2D(pool_size=(3,1), strides=(3,1))(x)\n",
        "    #      Block 2\n",
        "    x = layers.Conv2D(32, kernel_size=(3,1), strides=(1,1), padding='valid', use_bias=False)(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.AveragePooling2D(pool_size=(3,1), strides=(3,1))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    #      Block 3 - started\n",
        "    if vae:\n",
        "        latent_mean = layers.Dense(8, activation=None, name=\"latent_mean\")(x)\n",
        "        latent_log_var = layers.Dense(8, activation=None, name=\"latent_log_var\", kernel_initializer='zeros')(x)\n",
        "        z = Sampling()([latent_mean, latent_log_var])\n",
        "        return tf.keras.Model(inputs, [latent_mean, latent_log_var, z], name=name)\n",
        "\n",
        "    latent = layers.Dense(8, activation=None, name=\"latent\")(x)\n",
        "    return tf.keras.Model(inputs, latent, name=name)\n",
        "\n",
        "\n",
        "def build_cnn_decoder(name='cnn_decoder'):\n",
        "    \"\"\"\n",
        "    CNN decoder from the paper's bottom figure.\n",
        "    Takes a latent dimension (8) -> Dense -> Reshape -> Conv2DTranspose or Upsampling + Conv2D\n",
        "    to go back to shape (20,3,1) then maybe slice off padding if you want 19,3,1 final.\n",
        "    \"\"\"\n",
        "    #      Block 3 - remainder > Forming encoder input\n",
        "    latent_inputs = tf.keras.Input(shape=(8,), name=\"cnn_decoder_input\")\n",
        "    y = layers.Dense(64, activation=None)(latent_inputs)\n",
        "    y = layers.ReLU()(y)\n",
        "    y = layers.Reshape((2,1,32))(y)\n",
        "    #      Block 4\n",
        "    y = layers.Conv2D(32, kernel_size=(3,1), strides=(1,1), padding='same')(y)\n",
        "    y = layers.ReLU()(y)\n",
        "    y = layers.UpSampling2D(size=(3,1))(y)\n",
        "    y = layers.ZeroPadding2D(padding=((0,0),(1,1)))(y)\n",
        "    #      Block 5\n",
        "    y = layers.Conv2D(16, kernel_size=(3,1), strides=(1,1), padding='same')(y)\n",
        "    y = layers.ReLU()(y)\n",
        "    y = layers.UpSampling2D(size=(3,1))(y)\n",
        "    y = layers.ZeroPadding2D(padding=((1,0),(0,0)))(y)\n",
        "    #      Output\n",
        "    output = layers.Conv2D(1, kernel_size=(3,3), strides=(1,1), padding='same')(y)\n",
        "\n",
        "    return tf.keras.Model(latent_inputs, output, name=name)\n",
        "\n",
        "build_cnn_encoder().summary()\n",
        "build_cnn_decoder().summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "thMaO0Yj_K0f"
      },
      "outputs": [],
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class CNN_AE(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = build_cnn_encoder(name=\"cnn_encoder\")\n",
        "        self.decoder = build_cnn_decoder(name=\"cnn_decoder\")\n",
        "\n",
        "    def call(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class CNN_VAE(tf.keras.Model):\n",
        "    def __init__(self,eta_b=None, phi_b=None, input_shape=(19, 3, 1), latent_dim=3, beta=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = build_cnn_encoder(vae=True)\n",
        "        self.decoder = build_cnn_decoder()\n",
        "        self.beta = beta\n",
        "        self.eta_b = eta_b\n",
        "        self.phi_b = phi_b\n",
        "\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom train step\n",
        "        We do the normal thing\n",
        "        But also add the KL divergence term\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            # MSE over all spatial and channel dimensions\n",
        "            #reconstruction_loss = tf.reduce_mean(\n",
        "            #    tf.reduce_sum(tf.square(data - reconstruction), axis=(1, 2, 3))\n",
        "            #)\n",
        "            reconstruction_loss = full_MSE(data , reconstruction[:,:,:,0], self.eta_b, self.phi_b)\n",
        "\n",
        "            # KL divergence term\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.cast(tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)), tf.float32)\n",
        "\n",
        "            # Total loss with β weighting\n",
        "            total_loss = (1.0 - self.beta) * reconstruction_loss + self.beta * kl_loss\n",
        "\n",
        "        # Backprop\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # Update\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom test step for validation.\n",
        "        Same as train_step but no backprop.\n",
        "        \"\"\"\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "\n",
        "        # Same reconstruction loss\n",
        "        reconstruction_loss = full_MSE(data, reconstruction[:,:,:,0], self.eta_b, self.phi_b)\n",
        "\n",
        "        # KL divergence term\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.cast(tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)), tf.float32)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = (1.0 - self.beta) * reconstruction_loss + self.beta * kl_loss\n",
        "\n",
        "        # Update the state of the metrics\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "uytCvKog_K0g"
      },
      "source": [
        "## Load Data + Define training Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YSxbHWnP_K0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8544f4-bcaf-4f58-8d96-57b2bfd380f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pT Mean: 38.09187573090986, pT STD: 21.60700567476585\n",
            "η_b: 3.9999613761901855, φ_b: 3.141591787338257\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "# If you use the dockerfile this should just work\n",
        "dset = h5py.File('background_for_training.h5', 'r')\n",
        "dset = {key: dset[key][()] for key in dset.keys()}\n",
        "\"\"\"\n",
        "Contains keys:\n",
        "    Particles_Classes : 4 classes of Particles\n",
        "    Particles_Names : Names of the Particles\n",
        "    Particles : The data (n, 19,4)\n",
        "        19 : Indexes are\n",
        "            - 0 : Missing Transverse Energy\n",
        "            - 1:4 Up to 4 electrons\n",
        "            - 4:8 Up to 4 muons\n",
        "            - 8-18 Up to 10 jets\n",
        "        Subdimension 4 by idx:\n",
        "            - 0 : pT (transverse momentum)\n",
        "            - 1 : eta (pseudorapidity)\n",
        "            - 2 : phi (azimuthal angle)\n",
        "            - 3 : class (0=Nothing, 1=Met,2=electron,3=muon,4=jet)\n",
        "And when something doesnt make sense (ie [:,0,1:4]) its just zero\n",
        "\"\"\"\n",
        "data = dset['Particles']\n",
        "del dset\n",
        "# Do z score norm to aid in training : They dont specify how they made O(1) : And I assume they mean across all defined objects\n",
        "detected_bmap = (data[:,:,3] != 0) # Select defined entries\n",
        "mean_pt = tf.reduce_mean(data[detected_bmap, 0])\n",
        "std_pt = tf.math.reduce_std(data[detected_bmap, 0])\n",
        "data[:,:,0] = ((data[:,:,0] - mean_pt) / std_pt)\n",
        "\n",
        "\"\"\"\n",
        "They mention in the paper:\n",
        "To account for physical boundaries of η and φ, for those features a re-scaled tanh activation is used in the loss computation.\n",
        "\n",
        "So I assume this means we need to find the extent of the parameters (call it b ) and do tanh(x/b) prior to the loss function,\n",
        "I am not sure how one would go about implementing this for KL loss so Im going to do MSE only\n",
        "\"\"\"\n",
        "eta_b = np.maximum(np.max(data[detected_bmap, 1]), np.abs(np.min(data[detected_bmap, 1]))).astype(\"float32\")\n",
        "phi_b = np.maximum(np.max(data[detected_bmap, 2]), np.abs(np.min(data[detected_bmap, 2]))).astype(\"float32\")\n",
        "\n",
        "# I didnt add a custom layer to do the pT Transformation so need to do it manually for misc datasets\n",
        "print(f\"pT Mean: {mean_pt}, pT STD: {std_pt}\")\n",
        "print(f\"η_b: {eta_b}, φ_b: {phi_b}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-yHI_oVQ_K0h"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "train_split = int(0.5*len(data))\n",
        "val_split = int((0.4+0.5)*len(data))\n",
        "\n",
        "n = np.arange(len(data))\n",
        "np.random.shuffle(n)\n",
        "# Do splitting - Dont grab class indices\n",
        "train = data[n[:train_split],          :, :3]\n",
        "val   = data[n[train_split:val_split], :, :3]\n",
        "test  = data[n[val_split:],            :, :3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pEHVFt43_K0i"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "def train_dnn_ae(x_train, x_val, epochs=100, batch_size=1024):\n",
        "    \"\"\"\n",
        "    Create and train\n",
        "    \"\"\"\n",
        "    # Build\n",
        "    dnn_ae = DNN_AE(\n",
        "        input_dim=57, latent_dim=3\n",
        "        )\n",
        "\n",
        "    # Compile with Adam and masked MSE\n",
        "    loss_func = tf.function(partial(masked_mse, eta_b=eta_b, phi_b=phi_b))\n",
        "    dnn_ae.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=loss_func\n",
        "    )\n",
        "    # Make Callback\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=10, restore_best_weights=True # restore???\n",
        "        )\n",
        "    ]\n",
        "    # Fit\n",
        "    dnn_ae.fit(\n",
        "        x_train, x_train,\n",
        "        validation_data=(x_val, x_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    return dnn_ae\n",
        "\n",
        "def train_dnn_vae(x_train, x_val, epochs=100, batch_size=1024, beta=1.0):\n",
        "    \"\"\"\n",
        "    Create and train\n",
        "    \"\"\"\n",
        "    # Build\n",
        "    dnn_vae = DNN_VAE(eta_b=eta_b, phi_b=phi_b, input_dim=57, latent_dim=3, beta=beta)\n",
        "\n",
        "    # Compile with Adam and masked MSE\n",
        "    loss_func = tf.function(partial(masked_mse, eta_b=eta_b, phi_b=phi_b))\n",
        "    dnn_vae.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
        "        loss=loss_func\n",
        "    )\n",
        "    # Make Callback\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=10, restore_best_weights=True # restore???\n",
        "        )\n",
        "    ]\n",
        "    # Fit\n",
        "    dnn_vae.fit(\n",
        "        x_train, # It doesnt have y apparently its passed to train_step as (x, y) and then autoresolved to be *(x, y) for val it does need two I do not quite get why\n",
        "        validation_data=x_val,#(x_val,x_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    return dnn_vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x_plY8no_K0i"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "def train_cnn_ae(x_train, x_val, epochs=100, batch_size=1024):\n",
        "    \"\"\"\n",
        "    Create and train\n",
        "    \"\"\"\n",
        "    cnn_ae_model = CNN_AE(\n",
        "        # No Args\n",
        "    )\n",
        "    loss_func = tf.function(partial(full_MSE, eta_b=eta_b, phi_b=phi_b))\n",
        "    cnn_ae_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=loss_func\n",
        "    )\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=10, restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        "    cnn_ae_model.fit(\n",
        "        x_train, x_train,\n",
        "        validation_data=(x_val, x_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    return cnn_ae_model\n",
        "\n",
        "def train_cnn_vae(x_train, x_val, epochs=100, batch_size=1024, beta=1.0):\n",
        "    \"\"\"\n",
        "    Create and train\n",
        "    \"\"\"\n",
        "    cnn_vae_model = CNN_VAE( eta_b=eta_b, phi_b=phi_b, beta=beta )\n",
        "    cnn_vae_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss='mse' # I think this is just ignored since we have a custom train loop\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=10, restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        "    cnn_vae_model.fit(\n",
        "        x_train,\n",
        "        validation_data=x_val,#(x_val, x_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return cnn_vae_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3WxEJlQ_K0i"
      },
      "source": [
        "# Actual Training\n",
        "\n",
        "Note that there is a lot of duplicate code as the kernel often doesnt survive past more than one model trainings\n",
        "\n",
        "Also the saving\n",
        "\n",
        "It wants me to implement a get_config method for saving the model, I dont want to do that as this means it will also want me to make a load config and the probably 20 other things\n",
        "\n",
        "I just create a new uncompiled model copy the weigths over save that (and we only really need it to save those anyway) and that should be directly loadable, also it doesnt like hdf5 for saving anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GuyhXT8P_K0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5891acb3-8c39-41a4-ec53-14c5558fc94a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import os\\npath = '/Code/Replicate/Models'\\nif not os.path.exists(path):\\n    os.makedirs(path)\\nepochs = 100\\nbatch_size = 1024\\n\\ndnn_ae  = train_dnn_ae( train, val, epochs=epochs, batch_size=batch_size)\\n# This keeps failing\\n#dnn_ae.save(f'{path}/full_dnn_ae.keras')\\ntmp = DNN_AE(\\n        input_dim=57, latent_dim=3\\n        )\\ntmp.set_weights(dnn_ae.get_weights())\\ntmp.save(f'{path}/full_dnn_ae.keras')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "'''import os\n",
        "path = '/Code/Replicate/Models'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "epochs = 100\n",
        "batch_size = 1024\n",
        "\n",
        "dnn_ae  = train_dnn_ae( train, val, epochs=epochs, batch_size=batch_size)\n",
        "# This keeps failing\n",
        "#dnn_ae.save(f'{path}/full_dnn_ae.keras')\n",
        "tmp = DNN_AE(\n",
        "        input_dim=57, latent_dim=3\n",
        "        )\n",
        "tmp.set_weights(dnn_ae.get_weights())\n",
        "tmp.save(f'{path}/full_dnn_ae.keras')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6LASDpNF_K0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b32b6697-abbf-4ca9-804e-b3048def1f1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import os\\npath = '/Code/Replicate/Models'\\nif not os.path.exists(path):\\n    os.makedirs(path)\\nepochs = 100\\nbatch_size = 1024\\n\\ncnn_ae  = train_cnn_ae( train, val, epochs=epochs, batch_size=batch_size)\\n\\n#cnn_ae.save(f'{path}/full_cnn_ae.keras')\\ntmp = CNN_AE(\\n        )\\ntmp.set_weights(cnn_ae.get_weights())\\ntmp.save(f'{path}/full_cnn_ae.keras')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "'''import os\n",
        "path = '/Code/Replicate/Models'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "epochs = 100\n",
        "batch_size = 1024\n",
        "\n",
        "cnn_ae  = train_cnn_ae( train, val, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "#cnn_ae.save(f'{path}/full_cnn_ae.keras')\n",
        "tmp = CNN_AE(\n",
        "        )\n",
        "tmp.set_weights(cnn_ae.get_weights())\n",
        "tmp.save(f'{path}/full_cnn_ae.keras')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w36Aiue4_K0j"
      },
      "source": [
        "# Beta testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_OWMEGeN_K0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "f44b7533-c7e0-46e9-a433-396941edb15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1954/1954\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 25ms/step - kl_loss: 0.0167 - loss: 0.0167 - reconstruction_loss: 2.9526 - val_kl_loss: 3.8852e-04 - val_loss: 3.8852e-04 - val_reconstruction_loss: 2.9545\n",
            "Epoch 2/100\n",
            "\u001b[1m1954/1954\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 30ms/step - kl_loss: 3.0544e-04 - loss: 3.0544e-04 - reconstruction_loss: 2.9525 - val_kl_loss: 1.8194e-04 - val_loss: 1.8194e-04 - val_reconstruction_loss: 2.9550\n",
            "Epoch 3/100\n",
            "\u001b[1m1713/1954\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - kl_loss: 1.6867e-04 - loss: 1.6867e-04 - reconstruction_loss: 2.9516"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-094a549be285>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdnn_vae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dnn_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#dnn_vae.save(f'{path}/full_dnn_vae_beta{i}.keras')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-a4d761bdf910>\u001b[0m in \u001b[0;36mtrain_dnn_vae\u001b[0;34m(x_train, x_val, epochs, batch_size, beta)\u001b[0m\n\u001b[1;32m     52\u001b[0m     ]\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     dnn_vae.fit(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# It doesnt have y apparently its passed to train_step as (x, y) and then autoresolved to be *(x, y) for val it does need two I do not quite get why\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#(x_val,x_val),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Iterate e-3 to e0\n",
        "for i in [1]:\n",
        "    path = '/Code/Replicate/Models'\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    epochs = 100\n",
        "    batch_size = 1024\n",
        "\n",
        "    dnn_vae = train_dnn_vae(train, val, epochs=epochs, batch_size=batch_size, beta=i)\n",
        "\n",
        "    #dnn_vae.save(f'{path}/full_dnn_vae_beta{i}.keras')\n",
        "    tmp = DNN_VAE(eta_b=eta_b, phi_b=phi_b, input_dim=57, latent_dim=3, beta=i\n",
        "            )\n",
        "    tmp.set_weights(dnn_vae.get_weights())\n",
        "    tmp.save(f'{path}/full_dnn_vae_beta{i}.keras')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "44dUTtQq_K0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "44000a11-647b-443f-bebf-ac5f27e7da6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1954/1954\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828ms/step - kl_loss: 0.0028 - loss: 0.0028 - reconstruction_loss: 55.3309"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer \"cnn_encoder\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 19, 3) dtype=float32>, <tf.Tensor 'data_1:0' shape=(None, 19, 3) dtype=float32>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-95bae0df8279>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcnn_vae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cnn_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#cnn_vae.save(f'{path}/full_cnn_vae_beta{i}.keras')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-407b4f2ba524>\u001b[0m in \u001b[0;36mtrain_cnn_vae\u001b[0;34m(x_train, x_val, epochs, batch_size, beta)\u001b[0m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     ]\n\u001b[0;32m---> 44\u001b[0;31m     cnn_vae_model.fit(\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-06f309ed0bbd>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mSame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mno\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"cnn_encoder\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 19, 3) dtype=float32>, <tf.Tensor 'data_1:0' shape=(None, 19, 3) dtype=float32>]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Iterate e-3 to e0\n",
        "for i in [1]:\n",
        "    path = '/Code/Replicate/Models'\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    epochs = 100\n",
        "    batch_size = 1024\n",
        "\n",
        "    cnn_vae = train_cnn_vae(train, val, epochs=epochs, batch_size=batch_size, beta=i)\n",
        "\n",
        "    #cnn_vae.save(f'{path}/full_cnn_vae_beta{i}.keras')\n",
        "    tmp = CNN_VAE(eta_b=eta_b, phi_b=phi_b, beta=i\n",
        "            )\n",
        "    tmp.set_weights(cnn_vae.get_weights())\n",
        "    tmp.save(f'{path}/full_cnn_vae_beta{i}.keras')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNbd3iyv_K0j"
      },
      "source": [
        "# Results\n",
        "\n",
        "\n",
        "So Anomaly detection works the following ways:\n",
        "\n",
        "IO Based : Pass X through model and compute anomalie score, if it deviates strongly we have a Signal event - Since all data is background\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEw1yVmM_K0k"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iIwPe62_K0n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gotta Love Chat GPT I wrote none of this\n",
        "# ================================\n",
        "# ANOMALY SCORE COMPUTATION\n",
        "# ================================\n",
        "\n",
        "# === 1. IO-Based Anomaly Score ===\n",
        "def compute_io_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute input-output (IO) anomaly score based on reconstruction loss (MSE).\n",
        "    Used for: AE and VAE models\n",
        "\n",
        "    Args:\n",
        "        y_true (np.array): True input data [batch_size, height, width, channels]\n",
        "        y_pred (np.array): Reconstructed output from the model [batch_size, height, width, channels]\n",
        "\n",
        "    Returns:\n",
        "        np.array: Anomaly score for each sample based on MSE\n",
        "    \"\"\"\n",
        "    return np.square(y_true - y_pred)\n",
        "\n",
        "# === 2. KL-Based Anomaly Score ===\n",
        "def compute_kl_score(z_mean, z_log_var):\n",
        "    \"\"\"\n",
        "    Compute KL-based anomaly score based on latent space properties.\n",
        "    Used for: VAE models\n",
        "\n",
        "    Args:\n",
        "        z_mean (np.array): Mean of the latent space [batch_size, latent_dim]\n",
        "        z_log_var (np.array): Log variance of the latent space [batch_size, latent_dim]\n",
        "\n",
        "    Returns:\n",
        "        np.array: KL divergence score for each sample\n",
        "    \"\"\"\n",
        "    kl_div = -0.5 * np.sum(1 + z_log_var - np.square(z_mean) - np.exp(z_log_var), axis=1)\n",
        "    return kl_div\n",
        "\n",
        "# === 3. Rz-Based Anomaly Score ===\n",
        "def compute_rz_score(z_mean, z_log_var):\n",
        "    \"\"\"\n",
        "    Compute Rz-based anomaly score (Euclidean distance from origin in latent space).\n",
        "    Used for: VAE models\n",
        "\n",
        "    Args:\n",
        "        z_mean (np.array): Mean of the latent space [batch_size, latent_dim]\n",
        "        z_log_var (np.array): Log variance of the latent space [batch_size, latent_dim]\n",
        "\n",
        "    Returns:\n",
        "        np.array: Rz score for each sample\n",
        "    \"\"\"\n",
        "    rz = np.sqrt(np.sum(np.square(z_mean), axis=-1) + np.sum(np.exp(z_log_var), axis=-1))\n",
        "    return rz\n",
        "\n",
        "# Example input shapes (for reference):\n",
        "# y_true, y_pred: [batch_size, 19, 3, 1] — typical input size for CNN models\n",
        "# z_mean, z_log_var: [batch_size, latent_dim] — latent space output for VAEs\n",
        "\n",
        "# Example usage:\n",
        "# io_score = compute_io_score(y_true, y_pred)\n",
        "# kl_score = compute_kl_score(z_mean, z_log_var)\n",
        "# rz_score = compute_rz_score(z_mean, z_log_var)\n",
        "\n",
        "\n",
        "# THis is needed for data scaling that I did not include as a Layer\n",
        "mean_pt = 38.079907723150626\n",
        "std_pt = 25.747307900649552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4IjgA_O_K0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-k4Doxz_K0p"
      },
      "outputs": [],
      "source": [
        "from matplotlib.lines import Line2D\n",
        "# For ROC we also need a background set lets use 1e5 background events\n",
        "background = h5py.File('/Code/Dataset/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5', 'r')['Particles'][:1000000,:,:3]\n",
        "# ================================\n",
        "# PLOT OUTLINE BASED ON THE PAPER\n",
        "# ================================\n",
        "\n",
        "# === FIGURE II: ROC Curves for CNN and DNN Models ===\n",
        "# Data:\n",
        "# - x-axis → False Positive Rate (FPR)\n",
        "# - y-axis → True Positive Rate (TPR)\n",
        "# - Models:\n",
        "#   - CNN VAE (using IO, KL divergence, Rz scores)\n",
        "#   - DNN VAE (using IO, KL divergence, Rz scores)\n",
        "#   - CNN AE (using IO score)\n",
        "#   - DNN AE (using IO score)\n",
        "# Benchmark Models:\n",
        "# - LQ → bτ\n",
        "# - A → 4ℓ\n",
        "# Method:\n",
        "# - Compute ROC curves based on anomaly scores:\n",
        "#   - MSE for AEs\n",
        "#   - KL and Rz for VAEs\n",
        "#   - Plot separate ROC curves for CNN and DNN\n",
        "def figureII():\n",
        "    def do_roc(ax, score, y_true, text):\n",
        "        fpr, tpr, _ = roc_curve(y_true, score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        ax.plot(fpr, tpr, label=f'{text} (AUC = {roc_auc:.3f})')\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    # CNN LQ -> bτ\n",
        "    # Load SIGNAL ONLY Data\n",
        "    dset = h5py.File('/Code/Dataset/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5', 'r')\n",
        "    dset = dset['Particles'][:,:,:3]\n",
        "    labels = np.ones(dset.shape[0])\n",
        "\n",
        "    # Concatenate dset and background - Load as much bkground as data\n",
        "    n_points = dset.shape[0]\n",
        "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
        "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
        "\n",
        "    dset[:,:,0] = ((dset[:,:,0] - mean_pt) / std_pt)\n",
        "\n",
        "    CNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_vae_beta1.0.keras')\n",
        "    CNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_ae.keras')\n",
        "\n",
        "    cnn_io_vae = compute_io_score(dset, CNN_VAE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[0], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    cnn_dkl_vae = compute_kl_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[0], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    cnn_Rz_vae = compute_rz_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[0], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    cnn_io_ae = compute_io_score(dset, CNN_AE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "\n",
        "    # DNN LQ -> bτ\n",
        "    DNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_vae_beta1.0.keras')\n",
        "    DNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_ae.keras')\n",
        "\n",
        "    dnn_io_vae = compute_io_score(dset, DNN_VAE.predict(dset))\n",
        "    do_roc(ax[1], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    dnn_dkl_vae = compute_kl_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[1], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    dnn_Rz_vae = compute_rz_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[1], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    dnn_io_ae = compute_io_score(dset, DNN_AE.predict(dset))\n",
        "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "    # CNN A-> 4l\n",
        "    dset = h5py.File('/Code/Dataset/Ato4l_lepFilter_13TeV_filtered.h5', 'r')\n",
        "    dset = dset['Particles'][:,:,:3]\n",
        "    labels = np.ones(dset.shape[0])\n",
        "\n",
        "    # Concatenate dset and background\n",
        "    n_points = dset.shape[0]\n",
        "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
        "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
        "\n",
        "    cnn_io_vae = compute_io_score(dset, CNN_VAE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    cnn_dkl_vae = compute_kl_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[2], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    cnn_Rz_vae = compute_rz_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[2], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    cnn_io_ae = compute_io_score(dset, CNN_AE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[2], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "    # DNN A-> 4l\n",
        "    dnn_io_vae = compute_io_score(dset, DNN_VAE.predict(dset))\n",
        "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    dnn_dkl_vae = compute_kl_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[3], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    dnn_Rz_vae = compute_rz_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[3], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    dnn_io_ae = compute_io_score(dset, DNN_AE.predict(dset))\n",
        "    do_roc(ax[3], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "    # And formating\n",
        "    legend_text = [\n",
        "        \"CNN ROC LQ → bτ\",\n",
        "        \"DNN ROC LQ → bτ\",\n",
        "        \"CNN ROC A → 4ℓ\",\n",
        "        \"DNN ROC A → 4ℓ\"\n",
        "    ]\n",
        "    for i, a in enumerate(ax):\n",
        "        a.set_xlim(1e-6, 1)\n",
        "        a.set_ylim(1e-6, 1)\n",
        "        a.set_xscale('log')\n",
        "        a.set_yscale('log')\n",
        "        a.set_xlabel('False Positive Rate')\n",
        "        a.set_ylabel('True Positive Rate')\n",
        "        a.set_xticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
        "        a.set_yticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
        "        a.grid(False)\n",
        "        a.plot([1e-6, 1], [1e-6, 1], color='grey', linestyle='--', alpha=0.4)\n",
        "        a.axvline(x=1e-5, color='red', linestyle='--', label='Label')\n",
        "        # Add Legend Handle\n",
        "        handles, labels = a.get_legend_handles_labels()\n",
        "        # Create the custom text as a Line2D object\n",
        "        extra = Line2D([0], [0], color='none', label='')\n",
        "        # Insert at the beginning of the list\n",
        "        handles.insert(0, extra)\n",
        "        labels.insert(0, legend_text[i])\n",
        "        a.legend(handles=handles, labels=labels, loc='lower right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "figureII()\n",
        "\n",
        "\n",
        "# === FIGURE III: Additional ROC Curves for CNN and DNN Models ===\n",
        "# Data:\n",
        "# - x-axis → False Positive Rate (FPR)\n",
        "# - y-axis → True Positive Rate (TPR)\n",
        "# - Models:\n",
        "#   - CNN VAE (using IO, KL divergence, Rz scores)\n",
        "#   - DNN VAE (using IO, KL divergence, Rz scores)\n",
        "#   - CNN AE (using IO score)\n",
        "#   - DNN AE (using IO score)\n",
        "# Benchmark Models:\n",
        "# - h± → τν\n",
        "# - h0 → ττ\n",
        "# Method:\n",
        "# - Same as Figure II, but using different benchmark scenarios\n",
        "\n",
        "def figureIII():\n",
        "    def do_roc(ax, score, y_true, text):\n",
        "        fpr, tpr, _ = roc_curve(y_true, score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        ax.plot(fpr, tpr, label=f'{text} (AUC = {roc_auc:.3f})')\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    # CNN LQ -> bτ\n",
        "    # Load SIGNAL ONLY Data\n",
        "    dset = h5py.File('/Code/Dataset/hChToTauNu_13TeV_PU20_filtered.h5', 'r')\n",
        "    dset = dset['Particles'][:,:,:3]\n",
        "    labels = np.ones(dset.shape[0])\n",
        "\n",
        "    # Concatenate dset and background - Load as much bkground as data\n",
        "    n_points = dset.shape[0]\n",
        "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
        "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
        "\n",
        "    dset[:,:,0] = ((dset[:,:,0] - mean_pt) / std_pt)\n",
        "\n",
        "    CNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_vae_beta1.0.keras')\n",
        "    CNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_ae.keras')\n",
        "\n",
        "    cnn_io_vae = compute_io_score(dset, CNN_VAE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[0], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    cnn_dkl_vae = compute_kl_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[0], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    cnn_Rz_vae = compute_rz_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[0], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    cnn_io_ae = compute_io_score(dset, CNN_AE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "\n",
        "    # DNN LQ -> bτ\n",
        "    DNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_vae_beta1.0.keras')\n",
        "    DNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_ae.keras')\n",
        "\n",
        "    dnn_io_vae = compute_io_score(dset, DNN_VAE.predict(dset))\n",
        "    do_roc(ax[1], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    dnn_dkl_vae = compute_kl_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[1], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    dnn_Rz_vae = compute_rz_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[1], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    dnn_io_ae = compute_io_score(dset, DNN_AE.predict(dset))\n",
        "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "    # CNN A-> 4l\n",
        "    dset = h5py.File('/Code/Dataset/hToTauTau_13TeV_PU20_filtered.h5', 'r')\n",
        "    dset = dset['Particles'][:,:,:3]\n",
        "    labels = np.ones(dset.shape[0])\n",
        "\n",
        "    # Concatenate dset and background\n",
        "    n_points = dset.shape[0]\n",
        "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
        "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
        "\n",
        "    cnn_io_vae = compute_io_score(dset, CNN_VAE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    cnn_dkl_vae = compute_kl_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[2], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    cnn_Rz_vae = compute_rz_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[2], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    cnn_io_ae = compute_io_score(dset, CNN_AE.predict(dset)[:,:,:,0])\n",
        "    do_roc(ax[2], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "    # DNN A-> 4l\n",
        "    dnn_io_vae = compute_io_score(dset, DNN_VAE.predict(dset))\n",
        "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
        "\n",
        "    dnn_dkl_vae = compute_kl_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[3], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
        "\n",
        "    dnn_Rz_vae = compute_rz_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
        "    do_roc(ax[3], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
        "\n",
        "    dnn_io_ae = compute_io_score(dset, DNN_AE.predict(dset))\n",
        "    do_roc(ax[3], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
        "\n",
        "\n",
        "    # And formating\n",
        "    legend_text = [\n",
        "        \"CNN ROC h± → τν\",\n",
        "        \"DNN ROC h± → τν\",\n",
        "        \"CNN ROC h0 → ττ\",\n",
        "        \"DNN ROC h0 → ττ\"\n",
        "    ]\n",
        "    for i, a in enumerate(ax):\n",
        "        a.set_xlim(1e-6, 1)\n",
        "        a.set_ylim(1e-6, 1)\n",
        "        a.set_xscale('log')\n",
        "        a.set_yscale('log')\n",
        "        a.set_xlabel('False Positive Rate')\n",
        "        a.set_ylabel('True Positive Rate')\n",
        "        a.set_xticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
        "        a.set_yticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
        "        a.grid(False)\n",
        "        a.plot([1e-6, 1], [1e-6, 1], color='grey', linestyle='--', alpha=0.4)\n",
        "        a.axvline(x=1e-5, color='red', linestyle='--', label='Label')\n",
        "        # Add Legend Handle\n",
        "        handles, labels = a.get_legend_handles_labels()\n",
        "        # Create the custom text as a Line2D object\n",
        "        extra = Line2D([0], [0], color='none', label='')\n",
        "        # Insert at the beginning of the list\n",
        "        handles.insert(0, extra)\n",
        "        labels.insert(0, legend_text[i])\n",
        "        a.legend(handles=handles, labels=labels, loc='lower right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# figureIII()\n",
        "\n",
        "# === FIGURE IV: TPR Ratio vs. Bit Width for AE Models ===\n",
        "# Data:\n",
        "# - x-axis → Bit width (2, 4, 6, 8, 10, 12, 14, 16)\n",
        "# - y-axis → TPR ratio (TPR for quantized model / TPR for baseline model)\n",
        "# - Models:\n",
        "#   - CNN AE (top left)\n",
        "#   - DNN AE (top right)\n",
        "#   - CNN AE (bottom left, QAT)\n",
        "#   - DNN AE (bottom right, QAT)\n",
        "# Method:\n",
        "# - Use MSE-based anomaly score for AE models\n",
        "# - Compute TPR ratio at FPR = 10⁻⁵\n",
        "\n",
        "# === FIGURE V: TPR Ratio vs. Bit Width for VAE Models ===\n",
        "# Data:\n",
        "# - x-axis → Bit width (2, 4, 6, 8, 10, 12, 14, 16)\n",
        "# - y-axis → TPR ratio (TPR for quantized model / TPR for baseline model)\n",
        "# - Models:\n",
        "#   - CNN VAE (top left)\n",
        "#   - DNN VAE (top right)\n",
        "#   - CNN VAE (bottom left, QAT)\n",
        "#   - DNN VAE (bottom right, QAT)\n",
        "# Method:\n",
        "# - Use KL-based anomaly score for VAE models\n",
        "# - Compute TPR ratio at FPR = 10⁻⁵\n",
        "\n",
        "# === TABLE I: Performance Table at Floating-Point Precision ===\n",
        "# Data:\n",
        "# - Models:\n",
        "#   - CNN AE, CNN VAE\n",
        "#   - DNN AE, DNN VAE\n",
        "# - AD Scores:\n",
        "#   - IO (MSE)\n",
        "#   - KL divergence\n",
        "#   - Rz score\n",
        "# - Benchmark Models:\n",
        "#   - LQ → bτ\n",
        "#   - A → 4ℓ\n",
        "#   - h± → τν\n",
        "#   - h0 → ττ\n",
        "# - Metrics:\n",
        "#   - AUC\n",
        "#   - TPR @ FPR = 10⁻⁵\n",
        "\n",
        "# === TABLE II: Performance of Quantized and Pruned Models ===\n",
        "# Data:\n",
        "# - Models:\n",
        "#   - CNN AE (QAT 4-bit)\n",
        "#   - CNN VAE (PTQ 8-bit)\n",
        "#   - DNN AE (QAT 8-bit)\n",
        "#   - DNN VAE (PTQ 8-bit)\n",
        "# - AD Scores:\n",
        "#   - IO (MSE)\n",
        "#   - KL divergence\n",
        "#   - Rz score\n",
        "# - Metrics:\n",
        "#   - AUC\n",
        "#   - TPR @ FPR = 10⁻⁵\n",
        "\n",
        "# === TABLE III: FPGA Resource Usage and Latency ===\n",
        "# Data:\n",
        "# - Models:\n",
        "#   - CNN AE (QAT 4-bit)\n",
        "#   - CNN VAE (PTQ 8-bit)\n",
        "#   - DNN AE (QAT 8-bit)\n",
        "#   - DNN VAE (PTQ 8-bit)\n",
        "# - Metrics:\n",
        "#   - DSP usage (%)\n",
        "#   - LUT usage (%)\n",
        "#   - Flip-Flop usage (%)\n",
        "#   - BRAM usage (%)\n",
        "#   - Latency (ns)\n",
        "#   - Initiation interval (ns)\n",
        "\n",
        "# === TABLE IV: FPGA Resource Usage for DNN AE ===\n",
        "# Data:\n",
        "# - Model:\n",
        "#   - DNN VAE (PTQ 8-bit)\n",
        "# - Metrics:\n",
        "#   - DSP usage (%)\n",
        "#   - LUT usage (%)\n",
        "#   - Flip-Flop usage (%)\n",
        "#   - BRAM usage (%)\n",
        "#   - Latency (ns)\n",
        "#   - Initiation interval (ns)\n",
        "# - Results are for a Xilinx V7-690 FPGA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaewHnXv_K0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ3aZjiP_K0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1MuLlww_K0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmgZ1VG8_K0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lHUW8Ud_K0p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from functools import partial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESe0YNfX_K0p"
      },
      "source": [
        "# Quantize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-EtQ4sk_K0p"
      },
      "outputs": [],
      "source": [
        "model.encoder.predict(data[:10, :, :3, None])[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J56mqXbc_K0p"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}