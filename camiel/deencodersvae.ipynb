{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e7hF1vYTTZdy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 20:03:31.120137: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 20:03:31.194613: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-28 20:03:33.930386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/Software/users/modules/9/software/anaconda3/2023.03/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.11.909) or chardet (4.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers, Model\n",
    "from keras import ops\n",
    "from functools import partial\n",
    "\n",
    "@tf.function\n",
    "def con(x, y):\n",
    "    \n",
    "  return tf.stack([x, y], axis=2)\n",
    "\n",
    "@tf.function\n",
    "def msloss(ytrue, ypred):\n",
    "  \"\"\"\n",
    "  modified loss function with rescaling\n",
    "\n",
    "  needs some work vor vae\n",
    "  \"\"\"\n",
    "  #ytrue[:] = ytrue[:,tf.where(tf.sum(ytrue[:,:]))]\n",
    "  #ypred[:] = ypred[:,tf.where(tf.sum(ytrue[:,:]))]\n",
    "  #ytrue = layers.Input(shape=(19,3))(ytrue)\n",
    "  #ypred = layers.input(shape=(19,3))\n",
    "\n",
    "  yte = tf.tanh(ytrue[:,:,1])\n",
    "  ytp = tf.tanh(ytrue[:,:,2])\n",
    "\n",
    "  ype = tf.tanh(ypred[:,:,1])\n",
    "  ypp = tf.tanh(ypred[:,:,2])\n",
    "\n",
    "  #ytr = ypred[:,:,0]/\n",
    "  #ypr\n",
    "    \n",
    "  yt = tf.concat([ytrue[:,:,0], yte,ytp], axis=1)\n",
    "  yp = tf.concat([ypred[:,:,0], ype,ypp], axis=1)\n",
    "\n",
    "  return tf.reduce_mean(tf.reduce_sum(tf.square(yt-yp)))\n",
    "\n",
    "@tf.function\n",
    "def vmsloss(ytrue, ypred):\n",
    "  \"\"\"\n",
    "  modified loss function with rescaling\n",
    "\n",
    "  needs some work vor vae\n",
    "  \"\"\"\n",
    "  #ytrue[:] = ytrue[:,tf.where(tf.sum(ytrue[:,:]))]\n",
    "  #ypred[:] = ypred[:,tf.where(tf.sum(ytrue[:,:]))]\n",
    "  #ytrue = layers.Input(shape=(19,3))(ytrue)\n",
    "  #ypred = layers.input(shape=(19,3))\n",
    "\n",
    "  yte = tf.tanh(ytrue[1])\n",
    "  ytp = tf.tanh(ytrue[2])\n",
    "\n",
    "  ype = tf.tanh(ypred[1])\n",
    "  ypp = tf.tanh(ypred[2])\n",
    "\n",
    "  yt = tf.concat([ytrue[0], yte,ytp], axis=1)\n",
    "  yp = tf.concat([ypred[0], ype,ypp], axis=1)\n",
    "  if yp.shape != yt.shape:\n",
    "    yp = tf.reshape(yp, yt.shape)\n",
    "\n",
    "  return tf.reduce_mean(tf.reduce_sum(tf.square(yt-yp)))\n",
    "\n",
    "@tf.function\n",
    "def tmp(inputs, see):\n",
    "    \"\"\"For VAE implementation a lot of code is taken from\n",
    "    https://keras.io/examples/generative/vae/\n",
    "    \n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import ops\n",
    "    from keras import layers\n",
    "    class Sampling(layers.Layer):\n",
    "      Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "\n",
    "      def __init__(self, **kwargs):\n",
    "          super().__init__(**kwargs)\n",
    "          self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "      def call(self, inputs):\"\"\"\n",
    "    batch = ops.shape(inputs)[0]\n",
    "    dim = ops.shape(inputs)[1]\n",
    "    epsilon = keras.random.normal(shape=(batch, dim), seed=see)\n",
    "    return ops.exp(0.5 * inputs) * epsilon\n",
    "\n",
    "    #return Sampling\n",
    "\n",
    "#Sampling = tmp()\n",
    "\n",
    "def dnencoder(vae=0, latent_dim=3):\n",
    "  encoder_inputs = layers.Input(shape=(19,3,))\n",
    "  x = layers.BatchNormalization()(encoder_inputs)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(32)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.LeakyReLU()(x)\n",
    "  x = layers.Dense(16)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.LeakyReLU()(x)\n",
    "\n",
    "  if vae:\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    #tf.print(z_mean[0])\n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "  else:\n",
    "    x = layers.Dense(latent_dim)(x)\n",
    "    encoder = Model(encoder_inputs, x, name=\"encoder\")\n",
    "\n",
    "  return encoder\n",
    "\n",
    "def dndecoder(vae=1, latent_dim=3):\n",
    "\n",
    "  decoder_inputs = layers.Input(shape=(3,)) #keras doesn want any other shape than 3\n",
    "  x = layers.Dense(16)(decoder_inputs)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.LeakyReLU()(x)\n",
    "  x = layers.Dense(32)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.LeakyReLU()(x)\n",
    "  x = layers.Dense(19*3)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  decoder_outputs = layers.Reshape((19,3))(x)\n",
    "  return Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "def cnencoder(vae=0, latent_dim=8):\n",
    "  encoder_inputs = layers.Input(shape=(19,3,1))\n",
    "  x = layers.ZeroPadding2D(padding=(1,0))(encoder_inputs)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.Conv2D(16, kernel_size=(3, 3), strides=1, padding=\"valid\")(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.AveragePooling2D((3, 1))(x)\n",
    "  x = layers.Conv2D(32, kernel_size=(3, 1), strides=1, padding=\"valid\")(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.AveragePooling2D((3, 1))(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  if vae:\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "  else:\n",
    "    x = layers.Dense(latent_dim)(x)\n",
    "    encoder = Model(encoder_inputs, x, name=\"encoder\")\n",
    "  return encoder\n",
    "\n",
    "def cndecoder(vae=0, latent_dim=8):\n",
    "\n",
    "  decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "  #x = layers.BatchNormalization()\n",
    "  x = layers.Dense(64)(decoder_inputs)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.Reshape((2,1,32))(x)\n",
    "  x = layers.Conv2D(32,(3, 1), strides=1, padding='same')(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.UpSampling2D((3, 1))(x)\n",
    "  x = layers.ZeroPadding2D(((0,0),(1,1)))(x)\n",
    "  x = layers.Conv2D(16, (3, 1), strides=1, padding='same')(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.UpSampling2D((3, 1))(x)\n",
    "  x = layers.ZeroPadding2D(((1,0),(0,0)))(x)\n",
    "  decoder_outputs = layers.Conv2D(1, (3, 3), strides=1, padding='same')(x)\n",
    "  return Model(decoder_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vr42y3Bu-hWv"
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class AE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def call(self, inputs):\n",
    "        z= self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "beta = .01\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.seed=keras.random.SeedGenerator(1337)\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data[0])\n",
    "            z_mean = z_mean + tmp(z_log_var, self.seed)\n",
    "            reconstruction = self.decoder(z_mean)\n",
    "            reconstruction_loss = vmsloss(data[0], reconstruction)\n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "            \n",
    "            total_loss = (1-beta)*reconstruction_loss + beta*kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var = self.encoder(data[0])\n",
    "        reconstruction = self.decoder(z_mean)\n",
    "        reconstruction_loss = vmsloss(data[1], reconstruction)\n",
    "\n",
    "        # ✅ KL divergence\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.cast(tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1)), tf.float32)\n",
    "\n",
    "        # ✅ Total loss with β weighting\n",
    "        total_loss = (1.0 - beta) * reconstruction_loss + beta * kl_loss\n",
    "\n",
    "        # ✅ Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        #print(z_mean)\n",
    "        reconstruction = self.decoder(z_mean)\n",
    "        return reconstruction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eNnFXCRf0rP",
    "outputId": "31af3efc-749f-47cd-95b3-3cf1945e2321"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "\n",
    "dset = h5py.File('./content/background_for_training2.h5', 'r')\n",
    "#dset = {key: dset[key][()] for key in dset.keys()}\n",
    "\"\"\"\n",
    "Contains keys:\n",
    "    Particles_Classes : 4 classes of Particles\n",
    "    Particles_Names : Names of the Particles\n",
    "    Particles : The data (n, 19,4)\n",
    "        19 : Indexes are\n",
    "            - 0 : Missing Transverse Energy\n",
    "            - 1:4 Up to 4 electrons\n",
    "            - 4:8 Up to 4 muons\n",
    "            - 8-18 Up to 10 jets\n",
    "        Subdimension 4 by idx:\n",
    "            - 0 : pT (transverse momentum)\n",
    "            - 1 : eta (pseudorapidity)\n",
    "            - 2 : phi (azimuthal angle)\n",
    "            - 3 : class (0=Nothing, 1=Met,2=electron,3=muon,4=jet)\n",
    "And when something doesnt make sense (ie [:,0,1:4]) its just zero\n",
    "\"\"\"\n",
    "data = (dset['Particles'])[:,:,:3]\n",
    "del dset\n",
    "# Do z score norm to aid in training : They dont specify how they made O(1) : And I assume they mean across all defined objects\n",
    "#detected_bmap = (data[:,:,3] != 0) # Select defined entries\n",
    "#mean_pt = tf.reduce_mean(data[detected_bmap, 0])\n",
    "#std_pt = tf.math.reduce_std(data[detected_bmap, 0])\n",
    "#data[:,:,0] = ((data[:,:,0] - mean_pt) / std_pt)\n",
    "\n",
    "sloss = tf.function(partial(msloss))\n",
    "\n",
    "dn_ae = AE(dnencoder(), dndecoder())\n",
    "dn_ae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "\n",
    "cn_ae = AE(cnencoder(), cndecoder())\n",
    "cn_ae.compile(optimizer=keras.optimizers.Adam(), loss=sloss)\n",
    "\n",
    "dn_vae = VAE(dnencoder(vae=1), dndecoder(vae=1))\n",
    "dn_vae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "\n",
    "cn_vae = VAE(cnencoder(vae=1), cndecoder(vae=1))\n",
    "cn_vae.compile(optimizer=keras.optimizers.Adam(), loss=sloss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rNZz33UoK95l"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.5)\n",
    "test, val = train_test_split(test, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vFi3-oSaCqiw"
   },
   "outputs": [],
   "source": [
    "dn_ae.fit(train, train, validation_data=(test, test), epochs=100, batch_size=1024)\n",
    "dn_ae.save_weights('edn_ae_weights.weights.h5')\n",
    "#dn_ae.save('dn_ae_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con([1,2,3],[4,5,6])\n",
    "#dn_vae.compile(optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dn_vae = VAE(dnencoder(vae=1), dndecoder(vae=1))\n",
    "#dn_vae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "\n",
    "#dn_vae.predict(train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "5lH4sfChQ_Xp",
    "outputId": "4241463e-bf83-4162-9314-c4ae48ba2492"
   },
   "outputs": [],
   "source": [
    "dn_vae = VAE(dnencoder(vae=1), dndecoder(vae=1))\n",
    "dn_vae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "\n",
    "dn_vae.fit(train, train, validation_data=(test, test), epochs=100, batch_size=1024)\n",
    "dn_vae.save_weights('edn_vae_weights.weights.h5')\n",
    "#dn_vae.save('dn_vae_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "EunVbZpsLX69",
    "outputId": "685a7aed-a992-439d-c1fe-c2c50addc302"
   },
   "outputs": [],
   "source": [
    "cn_vae = VAE(cnencoder(vae=1), cndecoder(vae=1))\n",
    "cn_vae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "\n",
    "cn_vae.fit(train, train, validation_data=(test, test), epochs=100, batch_size=1024)\n",
    "cn_vae.save_weights('ecn_vae_weights.weights.h5')\n",
    "\n",
    "cn_ae.fit(train, train, validation_data=(test, test), epochs=100, batch_size=1024)\n",
    "cn_ae.save_weights('ecn_ae_weights.weights.h5')\n",
    "#cn_vae.save('cn_vae_model.keras')\n",
    "\n",
    "#cn_vae = VAE(cnencoder(vae=1), cndecoder())\n",
    "#cn_vae.compile(optimizer=keras.optimizers.Adam(), loss=sloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yB7Hc3Sk3na"
   },
   "source": [
    "## my plotting didn't work so using felix's for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gotta Love Chat GPT I wrote none of this \n",
    "# ================================\n",
    "# ANOMALY SCORE COMPUTATION \n",
    "# ================================\n",
    "\n",
    "# === 1. IO-Based Anomaly Score ===\n",
    "def compute_io_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute input-output (IO) anomaly score based on reconstruction loss (MSE).\n",
    "    Used for: AE and VAE models\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): True input data [batch_size, height, width, channels]\n",
    "        y_pred (np.array): Reconstructed output from the model [batch_size, height, width, channels]\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Anomaly score for each sample based on MSE\n",
    "    \"\"\"\n",
    "    return np.square(y_true - y_pred)\n",
    "\n",
    "# === 2. KL-Based Anomaly Score ===\n",
    "def compute_kl_score(z_mean, z_log_var):\n",
    "    \"\"\"\n",
    "    Compute KL-based anomaly score based on latent space properties.\n",
    "    Used for: VAE models\n",
    "    \n",
    "    Args:\n",
    "        z_mean (np.array): Mean of the latent space [batch_size, latent_dim]\n",
    "        z_log_var (np.array): Log variance of the latent space [batch_size, latent_dim]\n",
    "    \n",
    "    Returns:\n",
    "        np.array: KL divergence score for each sample\n",
    "    \"\"\"\n",
    "    kl_div = -0.5 * np.sum(1 + z_log_var - np.square(z_mean) - np.exp(z_log_var), axis=1)\n",
    "    # Replace inf \n",
    "    kl_div = np.nan_to_num(kl_div, nan=0.0, posinf=1e20, neginf=-1e20)\n",
    "    return kl_div\n",
    "\n",
    "# === 3. Rz-Based Anomaly Score ===\n",
    "def compute_rz_score(z_mean, z_log_var):\n",
    "    \"\"\"\n",
    "    Compute Rz-based anomaly score (Euclidean distance from origin in latent space).\n",
    "    Used for: VAE models\n",
    "    \n",
    "    Args:\n",
    "        z_mean (np.array): Mean of the latent space [batch_size, latent_dim]\n",
    "        z_log_var (np.array): Log variance of the latent space [batch_size, latent_dim]\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Rz score for each sample\n",
    "    \"\"\"\n",
    "    rz = np.sqrt(np.sum(np.square(z_mean), axis=-1) + np.sum(np.exp(z_log_var), axis=-1))\n",
    "    rz = np.nan_to_num(rz, nan=0.0, posinf=1e20, neginf=-1e20)\n",
    "    return rz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Software/users/modules/9/software/anaconda3/2023.03/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Software/users/modules/9/software/anaconda3/2023.03/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 38 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "path= './content/'\n",
    "\n",
    "dn_vae = VAE(dnencoder(vae=1), dndecoder(vae=1))\n",
    "dn_vae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "#dn_vae.fit(train[:2], train[:2], batch_size=1)\n",
    "dn_vae.load_weights('./rdn_vae_weights.weights.h5')\n",
    "\n",
    "cn_vae = VAE(cnencoder(vae=1), cndecoder(vae=1))\n",
    "cn_vae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "#cn_vae.fit(train[:2], train[:2], batch_size=1)\n",
    "cn_vae.load_weights('./rcn_vae_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxcFInA8qPeu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 2921.0601\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 2678.2769\n",
      "\u001b[1m21284/21284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 882us/step\n",
      "\u001b[1m21284/21284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step\n",
      "\u001b[1m21284/21284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 828us/step\n",
      "\u001b[1m21284/21284\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2ms/step\n",
      "\u001b[1m47517/47517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 892us/step\n",
      "\u001b[1m47517/47517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import h5py\n",
    "\n",
    "#dn_ae = AE(dnencoder(), dndecoder())\n",
    "#dn_ae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "#dn_ae.fit(train[:2], train[:2], batch_size=1)\n",
    "#dn_ae.load_weights('./dn_ae_weights.weights.h5')\n",
    "\n",
    "#cn_ae = AE(cnencoder(), cndecoder())\n",
    "#cn_ae.compile(optimizer=\"Adam\", loss=sloss)\n",
    "#cn_ae.fit(train[:2], train[:2], batch_size=1)\n",
    "#cn_ae.load_weights('./content/cn_ae_weights.weights.h5')\n",
    "\n",
    "# For ROC we also need a background set lets use 1e5 background events\n",
    "background = h5py.File('./content/background_for_training2.h5', 'r')['Particles'][:,:,:3][:1000000,:,:3]\n",
    "# ================================\n",
    "# PLOT OUTLINE BASED ON THE PAPER\n",
    "# ================================\n",
    "def compute_io_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute input-output (IO) anomaly score based on reconstruction loss (MSE).\n",
    "    Used for: AE and VAE models\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True input data [batch_size, height, width, channels]\n",
    "        y_pred (np.array): Reconstructed output from the model [batch_size, height, width, channels]\n",
    "\n",
    "    Returns:\n",
    "        np.array: Anomaly score for each sample based on MSE\n",
    "    \"\"\"\n",
    "    y_pred = tf.reshape(y_pred, y_true.shape)\n",
    "    return np.square(y_true - y_pred)\n",
    "# === FIGURE II: ROC Curves for CNN and DNN Models ===\n",
    "# Data:\n",
    "# - x-axis → False Positive Rate (FPR)\n",
    "# - y-axis → True Positive Rate (TPR)\n",
    "# - Models:\n",
    "#   - CNN VAE (using IO, KL divergence, Rz scores)\n",
    "#   - DNN VAE (using IO, KL divergence, Rz scores)\n",
    "#   - CNN AE (using IO score)\n",
    "#   - DNN AE (using IO score)\n",
    "# Benchmark Models:\n",
    "# - LQ → bτ\n",
    "# - A → 4ℓ\n",
    "# Method:\n",
    "# - Compute ROC curves based on anomaly scores:\n",
    "#   - MSE for AEs\n",
    "#   - KL and Rz for VAEs\n",
    "#   - Plot separate ROC curves for CNN and DNN\n",
    "def figureII():\n",
    "    def do_roc(ax, score, y_true, text):\n",
    "        fpr, tpr, _ = roc_curve(y_true, score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, label=f'{text} (AUC = {roc_auc:.3f})')\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    # CNN LQ -> bτ\n",
    "    #Load SIGNAL ONLY Data\n",
    "    dset = h5py.File('./content/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5', 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    #Concatenate dset and background - Load as much bkground as data\n",
    "    n_points = dset.shape[0]\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    #dset[:,:,0] = ((dset[:,:,0] - mean_pt) / std_pt)\n",
    "\n",
    "  #  CNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_vae_beta1.0.keras')\n",
    "  #  CNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_ae.keras')\n",
    "\n",
    "   # cnn_io_vae = compute_io_score(dset, CNN_VAE.predict(dset)[:,:,:,0])\n",
    "    #do_roc(ax[0], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "   # cnn_dkl_vae = compute_kl_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
    "    #do_roc(ax[0], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "   # cnn_Rz_vae = compute_rz_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
    "    #do_roc(ax[0], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "   # cnn_io_ae = compute_io_score(dset, CNN_AE.predict(dset)[:,:,:,0])\n",
    "    #do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"leptoquark\")#\"IO AE\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_ae.predict(dset))\n",
    "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"leptoquark\")#\"IO AE\")\n",
    "    \n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"leptoquark\")#\"IO AE\")\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset))\n",
    "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"leptoquark\")#\"IO AE\")\n",
    "\n",
    "    dset = h5py.File('./content/hChToTauNu_13TeV_PU20_filtered.h5', 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    #Concatenate dset and background - Load as much bkground as data\n",
    "    n_points = dset.shape[0]\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"hChToTauNu\")#\"IO AE\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_ae.predict(dset))\n",
    "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"hChToTauNu\")\n",
    "    \n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"hChToTauNu\")#\"IO AE\")\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset))\n",
    "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"hChToTauNu\")#\"IO AE\")\n",
    "\n",
    "    dset = h5py.File('./content/Ato4l_lepFilter_13TeV_filtered.h5', 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    n_points = dset.shape[0]\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"Ato4l\")#\"IO AE\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_ae.predict(dset))\n",
    "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"Ato4l\")\n",
    "    \n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"Ato4l\")#\"IO AE\")\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset))\n",
    "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"Ato4l\")#\"IO AE\")\n",
    "\n",
    "    dset = h5py.File('./content/hToTauTau_13TeV_PU20_filtered.h5', 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    n_points = dset.shape[0]\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"hToTauTau\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_ae.predict(dset))\n",
    "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"hToTauTau\")\n",
    "    \n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"hToTauTau\")#\"IO AE\")\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset))\n",
    "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"hToTauTau\")#\"IO AE\")\n",
    "\n",
    "    # DNN LQ -> bτ\n",
    "    #DNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_vae_beta1.0.keras')\n",
    "    #DNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_ae.keras')\n",
    "    #dnn_ae.load_weights('/content/dnn.weights.h5')\n",
    "\n",
    "\n",
    "#    dnn_io_vae = compute_io_score(dset, DNN_VAE.predict(dset))\n",
    " #   do_roc(ax[1], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "  #  dnn_dkl_vae = compute_kl_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
    "   # do_roc(ax[1], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "   # dnn_Rz_vae = compute_rz_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
    "    #do_roc(ax[1], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    #dnn_io_ae = msloss(dset, dn_ae.predict(dset))\n",
    "    #do_roc(ax[1], tf.reduce_mean(dnn_io_ae), labels, \"IO AE\")\n",
    "\n",
    "    \"\"\"\n",
    "    # CNN A-> 4l\n",
    "    dset = h5py.File('Ato4l_lepFilter_13TeV_filtered.h5', 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    # Concatenate dset and background\n",
    "    n_points = dset.shape[0]\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, CNN_VAE.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    cnn_dkl_vae = compute_kl_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[2], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    cnn_Rz_vae = compute_rz_score(*CNN_VAE.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[2], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, CNN_AE.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[2], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "    # DNN A-> 4l\n",
    "    dnn_io_vae = compute_io_score(dset, DNN_VAE.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    dnn_dkl_vae = compute_kl_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[3], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    dnn_Rz_vae = compute_rz_score(*DNN_VAE.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[3], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # And formating\n",
    "    legend_text = [\n",
    "        \"CNN ROC LQ → bτ\",\n",
    "        \"DNN ROC LQ → bτ\",\n",
    "        \"CNN ROC A → 4ℓ\",\n",
    "        \"DNN ROC A → 4ℓ\"\n",
    "    ]\n",
    "    for i, a in enumerate(ax):\n",
    "        a.set_xlim(1e-6, 1)\n",
    "        a.set_ylim(1e-6, 1)\n",
    "        a.set_xscale('log')\n",
    "        a.set_yscale('log')\n",
    "        a.set_xlabel('False Positive Rate')\n",
    "        a.set_ylabel('True Positive Rate')\n",
    "        a.set_xticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
    "        a.set_yticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
    "        a.grid(False)\n",
    "        a.plot([1e-6, 1], [1e-6, 1], color='grey', linestyle='--', alpha=0.4)\n",
    "        a.axvline(x=1e-5, color='red', linestyle='--', label='Label')\n",
    "        # Add Legend Handle\n",
    "        handles, labels = a.get_legend_handles_labels()\n",
    "        # Create the custom text as a Line2D object\n",
    "        #extra = Line2D([0], [0], color='none', label='')\n",
    "        # Insert at the beginning of the list\n",
    "        #handles.insert(0, extra)\n",
    "        #labels.insert(0, legend_text[i])\n",
    "        a.legend(handles=handles, labels=labels, loc='lower right')\n",
    "    plt.savefig('figur1')\n",
    "    plt.show()\n",
    "\n",
    "figureII()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln5nGo1oZI_h"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "# For ROC we also need a background set lets use 1e5 background events\n",
    "background = h5py.File('{}background_for_training2.h5'.format(path), 'r')['Particles'][:1000000,:,:3]\n",
    "# ================================\n",
    "# PLOT OUTLINE BASED ON THE PAPER\n",
    "# ================================\n",
    "\n",
    "# === FIGURE II: ROC Curves for CNN and DNN Models ===\n",
    "# Data:\n",
    "# - x-axis → False Positive Rate (FPR)\n",
    "# - y-axis → True Positive Rate (TPR)\n",
    "# - Models:\n",
    "#   - CNN VAE (using IO, KL divergence, Rz scores)\n",
    "#   - DNN VAE (using IO, KL divergence, Rz scores)\n",
    "#   - CNN AE (using IO score)\n",
    "#   - DNN AE (using IO score)\n",
    "# Benchmark Models:\n",
    "# - LQ → bτ\n",
    "# - A → 4ℓ\n",
    "# Method:\n",
    "# - Compute ROC curves based on anomaly scores:\n",
    "#   - MSE for AEs\n",
    "#   - KL and Rz for VAEs\n",
    "#   - Plot separate ROC curves for CNN and DNN\n",
    "def figureII():\n",
    "    def do_roc(ax, score, y_true, text):\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(y_true, score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            ax.plot(fpr, tpr, label=f'{text} (AUC = {roc_auc:.3f})')\n",
    "        except:\n",
    "            pass\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    # CNN LQ -> bτ\n",
    "    # Load SIGNAL ONLY Data\n",
    "    dset = h5py.File('{}/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5'.format(path), 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    # Concatenate dset and background - Load as much bkground as data\n",
    "    n_points = dset.shape[0]\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    #dset[:,:,0] = ((dset[:,:,0] - mean_pt) / std_pt)\n",
    "\n",
    "    #CNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_vae_beta0.01.keras')\n",
    "    #CNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_ae.keras')\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[0], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    cnn_dkl_vae = compute_kl_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[0], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    cnn_Rz_vae = compute_rz_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[0], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_vae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "\n",
    "    # DNN LQ -> bτ\n",
    "    #DNN_VAE = tf.keras.models.load_model('content/full_dnn_vae_beta0.01.keras')\n",
    "    #DNN_AE = tf.keras.models.load_model('content/full_dnn_ae.keras')\n",
    "\n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    dnn_dkl_vae = compute_kl_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[1], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    dnn_Rz_vae = compute_rz_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[1], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "    # CNN A-> 4l\n",
    "    dset = h5py.File('{}Ato4l_lepFilter_13TeV_filtered.h5'.format(path), 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    # Concatenate dset and background\n",
    "    n_points = dset.shape[0]\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    cnn_dkl_vae = compute_kl_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[2], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    cnn_Rz_vae = compute_rz_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[2], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_ae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[2], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "    # DNN A-> 4l\n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    dnn_dkl_vae = compute_kl_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[3], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    dnn_Rz_vae = compute_rz_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[3], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "    # And formating\n",
    "    legend_text = [\n",
    "        \"CNN ROC LQ → bτ\",\n",
    "        \"DNN ROC LQ → bτ\",\n",
    "        \"CNN ROC A → 4ℓ\",\n",
    "        \"DNN ROC A → 4ℓ\"\n",
    "    ]\n",
    "    for i, a in enumerate(ax):\n",
    "        a.set_xlim(1e-6, 1)\n",
    "        a.set_ylim(1e-6, 1)\n",
    "        a.set_xscale('log')\n",
    "        a.set_yscale('log')\n",
    "        a.set_xlabel('False Positive Rate')\n",
    "        a.set_ylabel('True Positive Rate')\n",
    "        a.set_xticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
    "        a.set_yticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
    "        a.grid(False)\n",
    "        a.plot([1e-6, 1], [1e-6, 1], color='grey', linestyle='--', alpha=0.4)\n",
    "        a.axvline(x=1e-5, color='red', linestyle='--', label='Label')\n",
    "        # Add Legend Handle\n",
    "        handles, labels = a.get_legend_handles_labels()\n",
    "        # Create the custom text as a Line2D object\n",
    "        extra = Line2D([0], [0], color='none', label='')\n",
    "        # Insert at the beginning of the list\n",
    "        handles.insert(0, extra)\n",
    "        labels.insert(0, legend_text[i])\n",
    "        a.legend(handles=handles, labels=labels, loc='lower right')\n",
    "    plt.savefig('figur2')\n",
    "    plt.show()\n",
    "\n",
    "figureII()\n",
    "\n",
    "\n",
    "# === FIGURE III: Additional ROC Curves for CNN and DNN Models ===\n",
    "# Data:\n",
    "# - x-axis → False Positive Rate (FPR)\n",
    "# - y-axis → True Positive Rate (TPR)\n",
    "# - Models:\n",
    "#   - CNN VAE (using IO, KL divergence, Rz scores)\n",
    "#   - DNN VAE (using IO, KL divergence, Rz scores)\n",
    "#   - CNN AE (using IO score)\n",
    "#   - DNN AE (using IO score)\n",
    "# Benchmark Models:\n",
    "# - h± → τν\n",
    "# - h0 → ττ\n",
    "# Method:\n",
    "# - Same as Figure II, but using different benchmark scenarios\n",
    "\n",
    "def figureIII():\n",
    "    def do_roc(ax, score, y_true, text):\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(y_true, score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            ax.plot(fpr, tpr, label=f'{text} (AUC = {roc_auc:.3f})')\n",
    "        except:\n",
    "            pass\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    # CNN LQ -> bτ\n",
    "    # Load SIGNAL ONLY Data\n",
    "    dset = h5py.File('./content/hChToTauNu_13TeV_PU20_filtered.h5', 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    # Concatenate dset and background - Load as much bkground as data\n",
    "    n_points = np.minimum(dset.shape[0], background.shape[0])\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    #dset[:,:,0] = ((dset[:,:,0] - mean_pt) / std_pt)\n",
    "\n",
    "    #CNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_vae_beta0.01.keras')\n",
    "    #CNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_cnn_ae.keras')\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[0], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    cnn_dkl_vae = compute_kl_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[0], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    cnn_Rz_vae = compute_rz_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[0], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_ae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[0], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "\n",
    "    # DNN LQ -> bτ\n",
    "    #DNN_VAE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_vae_beta0.01.keras')\n",
    "    #DNN_AE = tf.keras.models.load_model('/Code/Replicate/Models/full_dnn_ae.keras')\n",
    "\n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    dnn_dkl_vae = compute_kl_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[1], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    dnn_Rz_vae = compute_rz_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[1], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[1], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "    # CNN A-> 4l\n",
    "    dset = h5py.File('./content/hToTauTau_13TeV_PU20_filtered.h5', 'r')\n",
    "    dset = dset['Particles'][:,:,:3]\n",
    "    labels = np.ones(dset.shape[0])\n",
    "\n",
    "    # Concatenate dset and background\n",
    "    n_points = np.minimum(dset.shape[0], background.shape[0])\n",
    "    dset = np.concatenate([dset, background[:n_points]], axis=0)\n",
    "    labels = np.concatenate([labels, np.zeros(n_points)])\n",
    "\n",
    "    cnn_io_vae = compute_io_score(dset, cn_vae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[2], cnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    cnn_dkl_vae = compute_kl_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[2], cnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    cnn_Rz_vae = compute_rz_score(*cn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[2], cnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    cnn_io_ae = compute_io_score(dset, cn_vae.predict(dset)[:,:,:,0])\n",
    "    do_roc(ax[2], cnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "    # DNN A-> 4l\n",
    "    dnn_io_vae = compute_io_score(dset, dn_vae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_vae.mean(axis=(1,2)), labels, \"IO VAE\")\n",
    "\n",
    "    dnn_dkl_vae = compute_kl_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[3], dnn_dkl_vae, labels, \"VAE D_{KL}\")\n",
    "\n",
    "    dnn_Rz_vae = compute_rz_score(*dn_vae.encoder.predict(dset)[0:2])\n",
    "    do_roc(ax[3], dnn_Rz_vae, labels, \"VAE R_Z\")\n",
    "\n",
    "    dnn_io_ae = compute_io_score(dset, dn_ae.predict(dset))\n",
    "    do_roc(ax[3], dnn_io_ae.mean(axis=(1,2)), labels, \"IO AE\")\n",
    "\n",
    "\n",
    "    # And formating\n",
    "    legend_text = [\n",
    "        \"CNN ROC h± → τν\",\n",
    "        \"DNN ROC h± → τν\",\n",
    "        \"CNN ROC h0 → ττ\",\n",
    "        \"DNN ROC h0 → ττ\"\n",
    "    ]\n",
    "    for i, a in enumerate(ax):\n",
    "        a.set_xlim(1e-6, 1)\n",
    "        a.set_ylim(1e-6, 1)\n",
    "        a.set_xscale('log')\n",
    "        a.set_yscale('log')\n",
    "        a.set_xlabel('False Positive Rate')\n",
    "        a.set_ylabel('True Positive Rate')\n",
    "        a.set_xticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
    "        a.set_yticks([10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0])\n",
    "        a.grid(False)\n",
    "        a.plot([1e-6, 1], [1e-6, 1], color='grey', linestyle='--', alpha=0.4)\n",
    "        a.axvline(x=1e-5, color='red', linestyle='--', label='Label')\n",
    "        # Add Legend Handle\n",
    "        handles, labels = a.get_legend_handles_labels()\n",
    "        # Create the custom text as a Line2D object\n",
    "        extra = Line2D([0], [0], color='none', label='')\n",
    "        # Insert at the beginning of the list\n",
    "        handles.insert(0, extra)\n",
    "        labels.insert(0, legend_text[i])\n",
    "        a.legend(handles=handles, labels=labels, loc='lower right')\n",
    "    plt.savefig('Figur3')\n",
    "    plt.show()\n",
    "\n",
    "figureIII()\n",
    "\n",
    "# === FIGURE IV: TPR Ratio vs. Bit Width for AE Models ===\n",
    "# Data:\n",
    "# - x-axis → Bit width (2, 4, 6, 8, 10, 12, 14, 16)\n",
    "# - y-axis → TPR ratio (TPR for quantized model / TPR for baseline model)\n",
    "# - Models:\n",
    "#   - CNN AE (top left)\n",
    "#   - DNN AE (top right)\n",
    "#   - CNN AE (bottom left, QAT)\n",
    "#   - DNN AE (bottom right, QAT)\n",
    "# Method:\n",
    "# - Use MSE-based anomaly score for AE models\n",
    "# - Compute TPR ratio at FPR = 10⁻⁵\n",
    "\n",
    "# === FIGURE V: TPR Ratio vs. Bit Width for VAE Models ===\n",
    "# Data:\n",
    "# - x-axis → Bit width (2, 4, 6, 8, 10, 12, 14, 16)\n",
    "# - y-axis → TPR ratio (TPR for quantized model / TPR for baseline model)\n",
    "# - Models:\n",
    "#   - CNN VAE (top left)\n",
    "#   - DNN VAE (top right)\n",
    "#   - CNN VAE (bottom left, QAT)\n",
    "#   - DNN VAE (bottom right, QAT)\n",
    "# Method:\n",
    "# - Use KL-based anomaly score for VAE models\n",
    "# - Compute TPR ratio at FPR = 10⁻⁵\n",
    "\n",
    "# === TABLE I: Performance Table at Floating-Point Precision ===\n",
    "# Data:\n",
    "# - Models:\n",
    "#   - CNN AE, CNN VAE\n",
    "#   - DNN AE, DNN VAE\n",
    "# - AD Scores:\n",
    "#   - IO (MSE)\n",
    "#   - KL divergence\n",
    "#   - Rz score\n",
    "# - Benchmark Models:\n",
    "#   - LQ → bτ\n",
    "#   - A → 4ℓ\n",
    "#   - h± → τν\n",
    "#   - h0 → ττ\n",
    "# - Metrics:\n",
    "#   - AUC\n",
    "#   - TPR @ FPR = 10⁻⁵\n",
    "\n",
    "# === TABLE II: Performance of Quantized and Pruned Models ===\n",
    "# Data:\n",
    "# - Models:\n",
    "#   - CNN AE (QAT 4-bit)\n",
    "#   - CNN VAE (PTQ 8-bit)\n",
    "#   - DNN AE (QAT 8-bit)\n",
    "#   - DNN VAE (PTQ 8-bit)\n",
    "# - AD Scores:\n",
    "#   - IO (MSE)\n",
    "#   - KL divergence\n",
    "#   - Rz score\n",
    "# - Metrics:\n",
    "#   - AUC\n",
    "#   - TPR @ FPR = 10⁻⁵\n",
    "\n",
    "# === TABLE III: FPGA Resource Usage and Latency ===\n",
    "# Data:\n",
    "# - Models:\n",
    "#   - CNN AE (QAT 4-bit)\n",
    "#   - CNN VAE (PTQ 8-bit)\n",
    "#   - DNN AE (QAT 8-bit)\n",
    "#   - DNN VAE (PTQ 8-bit)\n",
    "# - Metrics:\n",
    "#   - DSP usage (%)\n",
    "#   - LUT usage (%)\n",
    "#   - Flip-Flop usage (%)\n",
    "#   - BRAM usage (%)\n",
    "#   - Latency (ns)\n",
    "#   - Initiation interval (ns)\n",
    "\n",
    "# === TABLE IV: FPGA Resource Usage for DNN AE ===\n",
    "# Data:\n",
    "# - Model:\n",
    "#   - DNN VAE (PTQ 8-bit)\n",
    "# - Metrics:\n",
    "#   - DSP usage (%)\n",
    "#   - LUT usage (%)\n",
    "#   - Flip-Flop usage (%)\n",
    "#   - BRAM usage (%)\n",
    "#   - Latency (ns)\n",
    "#   - Initiation interval (ns)\n",
    "# - Results are for a Xilinx V7-690 FPGA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = h5py.File('./content/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5', 'r')\n",
    "dset = dset['Particles'][:,:,:3]\n",
    "labels = np.ones(dset.shape[0])\n",
    "#dset[:,:,0] = ((dset[:,:,0] - mean_pt) / std_pt)\n",
    "\n",
    "#dnn_ae = tf.keras.models.load_model('content/full_dnn_ae.keras')\n",
    "\n",
    "pred_sig, b = dn_vae.encoder.predict(dset)\n",
    "pred_back, b = dn_vae.encoder.predict(data[:, :3])\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "def plot_hull(ax, points, color):\n",
    "    hull = ConvexHull(points)\n",
    "    for simplex in hull.simplices:\n",
    "        ax.plot(points[simplex, 0], points[simplex, 1], points[simplex, 2], color=color)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(pred_sig[:,0], pred_sig[:,1], pred_sig[:,2], c='r', marker='o', label='Signal')\n",
    "#ax.scatter(pred_back[:,0], pred_back[:,1], pred_back[:,2], c='b', marker='o', label='Background')\n",
    "\n",
    "plot_hull(ax, pred_sig, 'red')\n",
    "plot_hull(ax, pred_back, 'blue')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('figur4')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(done)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
